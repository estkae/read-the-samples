If you've never used Redis, it is an open source, in-memory, key-value datastore. It can be thought of as a huge hashmap, with the ability to do a variety of different operations in an atomic fashion. One of the most common use cases for Redis is a LRU cache. In this post I'll describe the inner workings of the LRU algorithm in Redis, and how to use it effectively to speed up your applications.\n\nRedis and LRU\n\nRedis has a variety of different datatypes that you can store in a key. You can store a string, a list, a hash, a set, and even more exotic types like geospatial indexes and pub/sub channels.\n\nThe most common datatype you'll use in Redis is the hash, which is just a set of strings, stored under a single key. You'll also use the string type to store a variety of things, including session ids, and passwords.\n\nOne thing that you can do with a hash in Redis is use it as a cache. If you want to store a large dataset in memory, but want to do it with a very small footprint, you can use Redis as a hash-based cache.\n\nThe implementation of the LRU algorithm in Redis is more than just a simple feature, it is also the method that allows Redis to work as a cache. The LRU algorithm allows Redis to store your data in a very compact way, while also keeping it easily accessible.\n\nA Simple Example\n\nSuppose you have an application that stores a hash map of users, and each user has a name and a list of sessions. A session is just a string that represents an entry in the cache.\n\nIf we want to write the number of users in a session to the console, we can do something like this:\n\nIn redis-cli , type smembers *:user_name* , where user_name is the name of the user in the hash. This will return a list of sessions in this user's hash. Type smembers *:sessions* , where sessions is the key we used to store the sessions. This will return the number of sessions that we had in step 2.\n\nAs a quick note, in this example I am using "*" as a wildcard, this is done because I am not guaranteed that my keys have any characters after the colon.\n\nThis is the simplest example, but it works well to show how to use the Redis LRU algorithm to your advantage.\n\nIf you have a hash where each element represents a cached entry, your application is running well if each time you query the hash, the number of elements that you get is exactly the number of times you are using the hash in your application.\n\nTo explain why this is, let's do a simple analysis of a few different cases.\n\nImagine that the user "John Smith" has a hash, where each key is a session, and each value is a timestamp.\n\nIf we have 50 sessions for user John Smith, and we are querying the hash once per second, it should return the same number of elements every time.\n\nIf we have 50 sessions for user John Smith, and we are querying the hash once per second, it should return the same number of elements every time. If we have 10 sessions for user John Smith, and we are querying the hash once per second, it should return exactly 10 elements every time.\n\nIf we have 10 sessions for user John Smith, and we are querying the hash once per second, it should return exactly 10 elements every time. If we have 5 sessions for user John Smith, and we are querying the hash once per second, it should return either 5 or 6 elements every time, depending on whether the hash was queried during the first or second query.\n\nThis simple example is sufficient to show that the number of queries that are made to the cache are more important than the average number of items returned.\n\nThe number of times you query your cache is directly related to the amount of work your application is doing. If you have a lot of work to do, your application will make more queries to your cache. The way we can use this is by tuning the value of our cache to be more or less aggressive.\n\nThere are a few things that you can change to influence the aggressiveness of your cache. These are listed below.\n\nsize of the hash. How many elements the hash can have before we start dropping items.\n\nmax number of keys that can be in the hash\n\nmax number of elements per key that can be in the hash\n\nHere is an example that demonstrates how to increase the aggressiveness of a hash:\n\nredis> set maxmemory-policy volatile-lru maxmemory-samples 5 redis> set maxmemory-policy volatile-lru maxmemory-expand on\n\nIf you run the same example as above, and then run the two commands above, you will see a noticeable change. If you have a small hash, the new settings will be a dramatic change, if you have a large hash, the impact will be small.\n\nWhen using a hash, the number of elements per key is the key factor in determining the effectiveness of the LRU"